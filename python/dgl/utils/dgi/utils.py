"""Utilization functions."""
import torch
from dgl import DGLHeteroGraph
from dgl.utils import gather_pinned_tensor_rows

def get_new_arg_input(args, input_nodes, inference_graph, device, use_uva=False):
    """Get the new argument inputs indexed by input_nodes.

    Parameters
    ----------
    input_nodes : torch.Tensor
        Input nodes generated by dataloader.
    inference_graph : DGLHeteroGraph
        DGL graph object.
    device : torch.device
        Device to compute inference.
    use_uva : bool
        Whether use UVA.

    Returns
    ----------
    Tuple
        Arguments for inference computation.
    """
    new_args = ()
    for arg in args:
        if isinstance(arg, torch.Tensor):
            if arg.device == device:
                new_args += (arg[input_nodes],)
            elif use_uva:
                new_args += (gather_pinned_tensor_rows(arg, input_nodes),)
            else:
                new_args += (arg[input_nodes].to(device),)
        elif isinstance(arg, DGLHeteroGraph):
            new_args += (inference_graph.to(device),)
    return new_args

def update_ret_output(output_vals, rets, output_nodes, blocks):
    """Update output rets.

    Parameters
    ----------
    output_vals : tuple[torch.Tensor] or torch.Tensor
        Output values.
    rets : Tuple[torch.Tensor]
        Tensor holders for outputs.
    output_nodes : torch.Tensor
        Output nodes generated by dataloader.
    blocks : list[Blocks]
        Blocks generated by dataloader.

    Returns
    ----------
    Tuple
        rets.
    """
    if not isinstance(output_vals, tuple):
        output_vals = (output_vals,)
    for output_val, ret in zip(output_vals, rets):
        if output_val.size()[0] == blocks[0].num_dst_nodes():
            update_out_in_chunks(ret, output_nodes, output_val)
        else:
            raise RuntimeError("Can't determine return's type.")
    return rets

def update_out_in_chunks(ret, idx, val):
    """Update output in chunks.

    In pytorch implementation, transfer speed greatly decrease if the
    tensor size larger than 2^25. Here we update them in chunks to
    accelerate it.
    """
    memory_comsuption = 4 # TODO(Peiqi): we assume it float16, fix here later.
    for dim in range(1, len(val.shape)):
        memory_comsuption *= val.shape[dim]
    num_nodes = val.shape[0]
    num_node_in_chunks = (2^25) // memory_comsuption
    start, end = 0, 0
    while start < num_nodes:
        end = min(start + num_node_in_chunks, num_nodes)
        ret[idx[start:end]] = val[start:end].cpu()
        start = end
